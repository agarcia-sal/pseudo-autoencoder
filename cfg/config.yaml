defaults:
  - _self_
  - problem: pseudo_autoencoder
  - llm_client: openai
  - override hydra/output: local

hydra:
  job:
    name: ${problem.problem_name}
    chdir: True

# The chosen algorithm
# algorithm: greedy # other options are "direct_answer"
algorithm: greedy
model: gpt-4.1-mini # this is a hack way of solving this but this line should not be here, also, switching to gpt-4.1-mini because rate limiting
pipeline: autoencoder # options: autoencoder, cosmetic, classifier
dataset: leet_code # options: human_eval, leet_code
split: train
use_timestamp: False # set to True if you want to load in timestamps from other runs and not do one full run

# for direct_answer and for cosmetic decoder prompt:
previous_timestamp_autoencoder: 2025-09-18_21-00-18
previous_timestamp_cosmetic: 2025-09-18_21-00-18
previous_timestamp_classifier: 2025-09-18_21-00-18
prev_iter_encoder: 33
prev_iter_decoder: 34
prev_iter_cosmetic: 16
prev_iter_classifier: 16

# parameters for the data
autoencoder_version: v0.3.5 # v0.3.2 is a small version of v0.3.5, which is the hard version of v0.3.0
cosmetic_version: v0.1.0 # update this as more datasets for the cosmetic pipeline are created
classifier_version: v0.1.0

# for autoencoder:
readability_metric: avg_syllables_per_word # options are: avg_syllables_per_word and avg_word_length
near_miss_threshold: 0.8

# for classifier:
autoencoder_timestamp_train: 2025-11-18_14-39-59 # for cosmetic and classifier pipelines
autoencoder_timestamp_test: 2025-11-18_15-33-31 # for cosmetic and classifier pipelines
cosmetic_timestamp_train: 2025-11-18_16-14-40 # for classifier.
cosmetic_timestamp_test: 2025-11-18_16-50-35 # for classifier.

# Pre-processing for the pipeline
load_previous: False # is generally set to False
load_previous_timestamp: 2025-09-13_12-15-28
previous_last_iter: 1


# Main GA loop parameters
num_iterations: 32
starting_iteration: 1 # can be adjusted to continue a previous run
rounds: 2 
evolving_encoder: True
evolving_decoder: False
evolving_cosmetic: False
evolving_classifier: False

# Plotting:
plotting_pipeline: True